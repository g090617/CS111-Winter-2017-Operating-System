Files included:

lab2_add.c: This is the source code file for multithread testing

SortedList.h: Header file for SortedList.c

SortedList.c: Source code for sourted double linked list

lab2_list.c: Source code for implementing multithreaded insertion
and deletion operations for double linked list

t2.sh, test.sh: These are the test cases shell scripts to test
the lab2_add code

t3.sh: Test cases for lab2a linked list and for lab2b part 1

t4.sh: Test cases for lab2b part 3

t_list.sh: Test cases for lab2b part 4

Makefile: Compile source code, test source code, clean up extra
files and compress necessary files to a tar file

lab2_list.csv: data from test cases of lab2_list

lab2b_list.csv: data from test cases of part 3

lab2b_4list.csv data from test cases of part 4

profile.gperf: performance data for each function in the 
thread routine 

lab_2b_list.gp: gnuplot file to create plot based on the 
data from .csv files

lab2b-1.png: plot for thorughput of add and list with 
the threads

lab2b-2.png: the plot of the wait-for-lock time, and 
the average time per operation against the number of competing threads. 

lab2b-3.png: the plot of the multilist yield plot for both
mutex and spinlock vs the number of threads

lab2b-4.png: the plot of the multilist without yield plot for both
mutex vs the number of threads

lab2b-5.png: the plot of the multilist without yield plot for both
spinlock vs the number of threads



QUESTION 2.3.1 - Cycles in the basic implementation:
Where do you believe most of the cycles are spent in the 1 and 
2-thread tests (for both add and list)?  Why do you believe 
these to be the most expensive parts of the code?

Where do you believe most of the time/cycles are being spent 
in the high-thread spin-lock tests?

Where do you believe most of the time/cycles are being spent 
in the high-thread mutex tests?

a. Most of the cycles are spent in the add functions for add
and lookup and insert for delete. Because the number of thread 
is small so the other threads don't have to wait or spin. Thus
most of the operations are taken during adding and looking up
through the list. Because adding has to load the register and
store the data back and look up has to travers through the list.

b. I believe most of the cycles are being spent spinning when
the number of threads is large, since more threads are trying 
to access the critical section of the code, but only one 
thread can access the critical section. Therefore, most of 
the threads are spinning in the while loop waiting for the
lock to be available.

c. When donig work in mutex, most of the time are being spent
waiting for the lock to be available


QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the cycles 
when the spin-lock version of the list exerciser is run with 
a large number of threads?

Why does this operation become so expensive with large 
numbers of threads?

a. According to profile.gperf, most of the cycles, which is
around 59.9% spent on function slock, line 87 spent most 
of the time

b. When the number of threads is large, since more threads 
are tryingto access the critical section of the code, but 
only one thread can access the critical section. Therefore, 
most of the threads are spinning in the while loop waiting 
for the lock to be available.


QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs # threads) and 
the average wait-for-mutex time (vs #threads).  

Why does the average lock-wait time rise so dramatically 
with the number of contending threads?

Why does the completion time per operation rise (less 
dramatically) with the number of contending threads?

How is it possible for the wait time per operation to 
go up faster (or higher) than the completion time per operation?

a. Because when the number of threads increases,
the probability that other thread is holding the lock 
increases. Therefore, the probability that the lock
is acquired increases.

b. Because when the number of threads increases, it would
just yield to the end of the list queue and wait instead
of spinning like spinlock.

c. To let the wait time go up faster than the completion 
time per operation, increasing the number of threads by 
a significant amount can make it because when more threads
are blocked, more time they spend waiting.


QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods 
as a function of the number of lists.
Should the throughput continue increasing as the number of 
lists is further increased?  If not, explain why not.

It seems reasonable to suggest the throughput of an N-way 
partitioned list should be equivalent to the throughput of 
a single list with fewer (1/N) threads.  
Does this appear to be true in the above curves?  
If not, explain why not.


a. As we can see from the graph, as the number of sublists
increases, the increase of the throughput is less dramatic.
If the lists keep increasing, the throughput will finally
stop increasing. 

b. According to my graph, this does not seem to be the case.
The main reason should be when the number of list is few,
the time per operation is high , so the thorughput is low.
But as the number of lists increases, the probability that
one thread acquired the lock of the same lock for the same
list decrease, which means the time of one operation decrease
and cause the thorughput to increase.